{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение уровня сложности англоязычных фильмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26kl2hl64rUV"
   },
   "source": [
    "Нам поставлена задача разработать ML решение для автоматического определения уровня сложности англоязычных фильмов.\n",
    "\n",
    "Нам предоставлены следующие данные:\n",
    "- два pdf_файла, которые содержат списки слов, распределённые по сложности текста на группы A2, B1, B2, C1;\n",
    "- xlsx-файл, содержащий названия фильмов в одном столбце, и категорию сложности каждого фильма в другом столбце;\n",
    "- папку `Subtitles_all`, которая содержит пять папок `A2`, `B1`, `B2`, `C1` и `Subtitles`. Каждая папка содержит файлы в формате `.srt`, именем файла является название фильма, а содержание файла - текст субтитров фильма.\n",
    "\n",
    "Нам предстоит изучить предоставленные данные и разработать ML решение, которое будет предсказывать категорию сложности англоязычных фильмов по субтитрам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4bIDP4y4rUW"
   },
   "source": [
    "## Загрузка необходимых библиотек и инструментов. Загрузка предоставленных файлов и папок. Предобработка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc204IKH4rUW"
   },
   "source": [
    "### Загрузим необходимые библиотеки и инструменты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TilVFZor4rUa"
   },
   "source": [
    "### Обработка pdf-файлов - словарей, содержащих английский слова, разбитые на категории сложности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr-a61l-4rUb"
   },
   "source": [
    "**Из предоставленных файлов извлечём словари по категориям и сохраним в отдельных переменных.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T17:50:59.685494Z",
     "iopub.status.busy": "2023-08-15T17:50:59.684916Z",
     "iopub.status.idle": "2023-08-15T17:51:02.210922Z",
     "shell.execute_reply": "2023-08-15T17:51:02.210032Z",
     "shell.execute_reply.started": "2023-08-15T17:50:59.685447Z"
    },
    "id": "wbrvgLwX4rUc",
    "outputId": "cc6542e6-6924-4ca2-bcf7-6d99fd638b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словарь A2_words:\n",
      "Первые слова словаря:  ['ability', 'able', 'abroad', 'accept', 'accident']\n",
      "Последние слова словаря:  ['worst', 'wow', 'yet', 'yours', 'zero']\n",
      "\n",
      "Словарь B1_words:\n",
      "Первые слова словаря:  ['absolutely', 'access', 'accommodation', 'account', 'achievement']\n",
      "Последние слова словаря:  ['written', 'wrong', 'yard', 'young', 'youth']\n",
      "\n",
      "Словарь B2_words:\n",
      "Первые слова словаря:  ['abandon', 'absolute', 'academic', 'acceptable', 'accompany']\n",
      "Последние слова словаря:  ['workforce', 'workplace', 'workshop', 'worm', 'wrist']\n",
      "\n",
      "Словарь C1_words:\n",
      "Первые слова словаря:  ['abolish', 'abortion', 'absence', 'absent', 'absurd']\n",
      "Последние слова словаря:  ['worship', 'worthwhile', 'worthy', 'yell', 'yield']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_text_with_pypdf2(pdf_path):\n",
    "    text = ''\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_words_between(text, start_word, end_word):\n",
    "    lines = text.splitlines()\n",
    "    start_index = next((i for i, line in enumerate(lines) if line.startswith(start_word)), None)\n",
    "    end_index = next((i for i, line in enumerate(lines) if end_word in line), None)\n",
    "\n",
    "    if start_index is not None and end_index is not None and end_index > start_index:\n",
    "        extracted_lines = lines[start_index:end_index]\n",
    "        words = [line.split()[0] for line in extracted_lines]\n",
    "        if end_word in lines[end_index]:\n",
    "            words.append(end_word)\n",
    "        return words\n",
    "    return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path_1 = 'D:/Яндекс Практикум/17. Мастерская_2/Датасет/The_Oxford_3000_by_CEFR_level.pdf'\n",
    "    pdf_file_path_2 = 'D:/Яндекс Практикум/17. Мастерская_2/Датасет/The_Oxford_5000_by_CEFR_level.pdf'\n",
    "\n",
    "    # Extract text using PyPDF2 for the first PDF\n",
    "    text_pypdf2_1 = extract_text_with_pypdf2(pdf_file_path_1)\n",
    "\n",
    "    A2_words = extract_words_between(text_pypdf2_1, \"ability\", \"zero\")\n",
    "\n",
    "    print(\"Словарь A2_words:\")\n",
    "    print(\"Первые слова словаря: \", A2_words[:5])\n",
    "    print(\"Последние слова словаря: \", A2_words[-5:])\n",
    "    print()\n",
    "\n",
    "    B1_words = extract_words_between(text_pypdf2_1, \"absolutely\", \"youth\")\n",
    "\n",
    "    print(\"Словарь B1_words:\")\n",
    "    print(\"Первые слова словаря: \", B1_words[:5])\n",
    "    print(\"Последние слова словаря: \", B1_words[-5:])\n",
    "    print()\n",
    "\n",
    "    B2_words_1 = extract_words_between(text_pypdf2_1, \"abandon\", \"zone\")\n",
    "\n",
    "    text_pypdf2_2 = extract_text_with_pypdf2(pdf_file_path_2)\n",
    "\n",
    "    B2_words_2 = extract_words_between(text_pypdf2_2, \"absorb\", \"wrist\")\n",
    "\n",
    "    B2_words = B2_words_1 + B2_words_2\n",
    "\n",
    "    print(\"Словарь B2_words:\")\n",
    "    print(\"Первые слова словаря: \", B2_words[:5])\n",
    "    print(\"Последние слова словаря: \", B2_words[-5:])\n",
    "    print()\n",
    "\n",
    "    C1_words = extract_words_between(text_pypdf2_2, \"abolish\", \"yield\")\n",
    "\n",
    "    print(\"Словарь C1_words:\")\n",
    "    print(\"Первые слова словаря: \", C1_words[:5])\n",
    "    print(\"Последние слова словаря: \", C1_words[-5:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wIwZ5RG4rUd"
   },
   "source": [
    "### Создаём датафрейм, содержащий два столбца - название файла и текст субтитров из этого файла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8zDUhlv4rUd"
   },
   "source": [
    "**Импортируем файлы субтитров из папок, в которых они содержатся. Определяем кодировку файла, открываем файл и извлекаем текст субтитров, создаём датафрейм из названий файлов и субтитров, содержащихся во всех папках с субтитрами, попутно удаляем пустые строки, временные коды и номер субтитра.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T17:52:05.899256Z",
     "iopub.status.busy": "2023-08-15T17:52:05.898830Z",
     "iopub.status.idle": "2023-08-15T17:54:14.774601Z",
     "shell.execute_reply": "2023-08-15T17:54:14.773587Z",
     "shell.execute_reply.started": "2023-08-15T17:52:05.899225Z"
    },
    "id": "OxHd_M1b4rUd",
    "outputId": "1e87a653-e902-4ba8-da09-e4a7c5ebf160"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...  \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...  \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...  \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...  \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибки в следующих файлах:\n"
     ]
    }
   ],
   "source": [
    "# Функция для определения кодировки файла\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# Инициализируем пустой список для хранения данных о субтитрах\n",
    "subtitles_data = []\n",
    "errors = []\n",
    "\n",
    "# Путь к базовой папке с файлами SRT\n",
    "base_folder = 'D:/Яндекс Практикум/17. Мастерская_2/Датасет/Subtitles_all'\n",
    "\n",
    "# Цикл для обхода папок с файлами SRT\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Проверяем, что это директория\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if file_path.endswith(\".srt\"):  # Проверяем, что это файл SRT\n",
    "                try:\n",
    "                    # Определяем кодировку файла\n",
    "                    encoding = detect_encoding(file_path)\n",
    "\n",
    "                    # Открываем файл SRT с определенной кодировкой\n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        subtitle_lines = file.readlines()\n",
    "\n",
    "                    # Обрабатываем файл SRT и извлекаем текст субтитров\n",
    "                    subtitles = []\n",
    "                    current_subtitle = \"\"\n",
    "                    for line in subtitle_lines:\n",
    "                        line = line.strip()\n",
    "                        if line.isdigit() or line == \"\":\n",
    "                            # Пропускаем номер субтитра и пустые строки\n",
    "                            continue\n",
    "                        elif \"-->\" in line:\n",
    "                            # Пропускаем временные коды\n",
    "                            continue\n",
    "                        else:\n",
    "                            current_subtitle += \" \" + line\n",
    "\n",
    "                            if line.endswith(\".\"):\n",
    "                                # Конец субтитра, добавляем его в список\n",
    "                                subtitles.append(current_subtitle.strip())\n",
    "                                current_subtitle = \"\"\n",
    "\n",
    "                    # Добавляем данные из файла SRT в список\n",
    "                    if subtitles:\n",
    "                        subtitles_data.append({'file': filename[:-4], 'text': ' '.join(subtitles)})\n",
    "                except Exception as e:\n",
    "                    errors.append((file_path, e))\n",
    "\n",
    "# Создаем DataFrame из списка данных о субтитрах\n",
    "df = pd.DataFrame(subtitles_data)\n",
    "\n",
    "# Выводим первые строки DataFrame для проверки\n",
    "display(df.head())\n",
    "\n",
    "print(\"Ошибки в следующих файлах:\")\n",
    "for file_path, error in errors:\n",
    "    print(f\"{file_path}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T17:58:02.869755Z",
     "iopub.status.busy": "2023-08-15T17:58:02.869019Z",
     "iopub.status.idle": "2023-08-15T17:58:02.898828Z",
     "shell.execute_reply": "2023-08-15T17:58:02.897354Z",
     "shell.execute_reply.started": "2023-08-15T17:58:02.869702Z"
    },
    "id": "kCvLrQEa4rUe",
    "outputId": "59307e52-1898-46de-f0fd-89a9a9bbce92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 276 entries, 0 to 275\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   file    276 non-null    object\n",
      " 1   text    276 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9pMCF3Z4rUe"
   },
   "source": [
    "**На случайном файле смотрим, что субтитры импортировались нормально.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T17:59:26.370335Z",
     "iopub.status.busy": "2023-08-15T17:59:26.369826Z",
     "iopub.status.idle": "2023-08-15T17:59:26.377957Z",
     "shell.execute_reply": "2023-08-15T17:59:26.376756Z",
     "shell.execute_reply.started": "2023-08-15T17:59:26.370292Z"
    },
    "id": "etD_XAfE4rUf",
    "outputId": "f2c6dd4d-3945-41c9-fadc-c533379d176e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183    Created and Encoded by --  Bokutox -- of  www....\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['text'].sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tdOZMXq4rUf"
   },
   "source": [
    "### Импортируем файл xlsx, создадим датафрейм с названием файла и уровнем сложности текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:00:11.191323Z",
     "iopub.status.busy": "2023-08-15T18:00:11.190605Z",
     "iopub.status.idle": "2023-08-15T18:00:11.250672Z",
     "shell.execute_reply": "2023-08-15T18:00:11.249452Z",
     "shell.execute_reply.started": "2023-08-15T18:00:11.191282Z"
    },
    "id": "5gI9ikxu4rUf",
    "outputId": "327708ae-bdd7-4231-dbc8-017d79e6b011"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie   level\n",
       "0         10_Cloverfield_lane(2016)      B1\n",
       "1  10_things_I_hate_about_you(1999)      B1\n",
       "2              A_knights_tale(2001)      B2\n",
       "3              A_star_is_born(2018)      B2\n",
       "4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к файлу .xlsx\n",
    "file_path = 'D:/Яндекс Практикум/17. Мастерская_2/Датасет/movies_labels.xlsx'\n",
    "\n",
    "# Импорт таблицы Excel и создание DataFrame\n",
    "df_xlxs = pd.read_excel(file_path)\n",
    "\n",
    "# Вывод первых 5 строк DataFrame для проверки\n",
    "df_xlxs = df_xlxs.drop(['id'], axis=1)\n",
    "df_xlxs.columns = df_xlxs.columns.str.lower()\n",
    "df_xlxs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T18:17:08.550892Z",
     "iopub.status.busy": "2023-08-02T18:17:08.550157Z",
     "iopub.status.idle": "2023-08-02T18:17:08.559314Z",
     "shell.execute_reply": "2023-08-02T18:17:08.557709Z",
     "shell.execute_reply.started": "2023-08-02T18:17:08.550848Z"
    },
    "id": "LONNCFdy4rUg"
   },
   "source": [
    "**Некоторые файлы имеют не один, а два-три уровня сложности. Упростим таблицу - оставим только наименьший уровень сложности.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:00:20.825226Z",
     "iopub.status.busy": "2023-08-15T18:00:20.824042Z",
     "iopub.status.idle": "2023-08-15T18:00:20.844822Z",
     "shell.execute_reply": "2023-08-15T18:00:20.843339Z",
     "shell.execute_reply.started": "2023-08-15T18:00:20.825175Z"
    },
    "id": "UqmhIC0_4rUg",
    "outputId": "650ef32f-8173-40f2-a694-ee734ca5cc43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level\n",
       "0         10_Cloverfield_lane(2016)    B1\n",
       "1  10_things_I_hate_about_you(1999)    B1\n",
       "2              A_knights_tale(2001)    B2\n",
       "3              A_star_is_born(2018)    B2\n",
       "4                     Aladdin(1992)    A2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xlxs['level'] = df_xlxs['level'].str.slice(0, 2)\n",
    "df_xlxs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSUNlA9F4rUg"
   },
   "source": [
    "## Предобработка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewP3RY4k4rUg"
   },
   "source": [
    "### Обработаем текст субтитров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tpz4l3Lh4rUh"
   },
   "source": [
    "**Импортируем словарь, содержащий стоп-слова.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:08:09.709343Z",
     "iopub.status.busy": "2023-08-15T18:08:09.708805Z",
     "iopub.status.idle": "2023-08-15T18:08:09.721935Z",
     "shell.execute_reply": "2023-08-15T18:08:09.720666Z",
     "shell.execute_reply.started": "2023-08-15T18:08:09.709297Z"
    },
    "id": "Ug3UiS884rUh"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kns9cI224rUh"
   },
   "source": [
    "**Удаляем лишние символы, стоп-слова. Методом лемматизации приводим слова к словарной форме.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:08:40.345672Z",
     "iopub.status.busy": "2023-08-15T18:08:40.345107Z",
     "iopub.status.idle": "2023-08-15T18:08:40.361819Z",
     "shell.execute_reply": "2023-08-15T18:08:40.360240Z",
     "shell.execute_reply.started": "2023-08-15T18:08:40.345632Z"
    },
    "id": "8JOdXH814rUh"
   },
   "outputs": [],
   "source": [
    "# Функции предобработки и лемматизации текста\n",
    "del_n = re.compile('\\n')\n",
    "del_tags = re.compile('<[^>]*>')\n",
    "del_brackets = re.compile('\\([^)]*\\)')\n",
    "clean_text = re.compile('[^а-яa-z\\s]')\n",
    "del_spaces = re.compile('\\s{2,}')\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = del_n.sub(' ', str(text).lower())\n",
    "    text = del_tags.sub(' ', text)\n",
    "    text = del_brackets.sub('', text)\n",
    "    res_text = clean_text.sub('', text)\n",
    "    return del_spaces.sub(' ', res_text)\n",
    "\n",
    "def del_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_tokens = tuple(map(lambda x: x if x not in stop_words else '', word_tokenize(text)))\n",
    "    res_text = ' '.join(clean_tokens)\n",
    "    return res_text\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "def lemmatize(text):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    lemmatized_words = [lmtzr.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:08:44.298581Z",
     "iopub.status.busy": "2023-08-15T18:08:44.297900Z",
     "iopub.status.idle": "2023-08-15T18:11:01.410230Z",
     "shell.execute_reply": "2023-08-15T18:11:01.408934Z",
     "shell.execute_reply.started": "2023-08-15T18:08:44.298536Z"
    },
    "id": "lJTReT5k4rUi",
    "outputId": "b08fc676-334d-4d75-f84e-3bd40207639d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  little girl im policeman little girl dont afra...  \n",
       "1  boy mom right luck tell theyre poison uh there...  \n",
       "2  merle thats right hear bitch get problem bring...  \n",
       "3  nothing nothing always something didnt dad tea...  \n",
       "4  rick morgan dont know youre dont know hear may...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Применяем предобработку и лемматизацию к тексту\n",
    "df['processed_text'] = df['text'].apply(prepare_text)\n",
    "df['clean_text'] = df['processed_text'].apply(del_stopwords)\n",
    "df['lemmatized_text'] = df['clean_text'].apply(lemmatize)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V35-LlzX4rUi"
   },
   "source": [
    "**Переименуем столбец с названием файла**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:11:12.844132Z",
     "iopub.status.busy": "2023-08-15T18:11:12.843702Z",
     "iopub.status.idle": "2023-08-15T18:11:12.852805Z",
     "shell.execute_reply": "2023-08-15T18:11:12.851103Z",
     "shell.execute_reply.started": "2023-08-15T18:11:12.844098Z"
    },
    "id": "cQ8DQCqc4rUi"
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace('file', 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:11:16.218940Z",
     "iopub.status.busy": "2023-08-15T18:11:16.218396Z",
     "iopub.status.idle": "2023-08-15T18:11:16.254647Z",
     "shell.execute_reply": "2023-08-15T18:11:16.253077Z",
     "shell.execute_reply.started": "2023-08-15T18:11:16.218900Z"
    },
    "id": "2VdQh1jr4rUi",
    "outputId": "7a0c1671-969a-446b-b1fa-5db77cc98c73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  little girl im policeman little girl dont afra...  \n",
       "1  boy mom right luck tell theyre poison uh there...  \n",
       "2  merle thats right hear bitch get problem bring...  \n",
       "3  nothing nothing always something didnt dad tea...  \n",
       "4  rick morgan dont know youre dont know hear may...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7hhcTa94rUj"
   },
   "source": [
    "**Объединяем таблицы с текстом и категорией сложности по названию файла, создаём новую таблицу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:12:22.755933Z",
     "iopub.status.busy": "2023-08-15T18:12:22.755440Z",
     "iopub.status.idle": "2023-08-15T18:12:22.780003Z",
     "shell.execute_reply": "2023-08-15T18:12:22.778441Z",
     "shell.execute_reply.started": "2023-08-15T18:12:22.755894Z"
    },
    "id": "JpTbHkQ04rUj"
   },
   "outputs": [],
   "source": [
    "df_1 = pd.merge(df, df_xlxs, how ='outer', on ='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:12:37.279996Z",
     "iopub.status.busy": "2023-08-15T18:12:37.279508Z",
     "iopub.status.idle": "2023-08-15T18:12:37.298348Z",
     "shell.execute_reply": "2023-08-15T18:12:37.296903Z",
     "shell.execute_reply.started": "2023-08-15T18:12:37.279960Z"
    },
    "id": "P4uH4PoF4rUj",
    "outputId": "b1963e04-454f-4cf9-cf72-8882e94f1e87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text level  \n",
       "0  little girl im policeman little girl dont afra...    A2  \n",
       "1  boy mom right luck tell theyre poison uh there...    A2  \n",
       "2  merle thats right hear bitch get problem bring...    A2  \n",
       "3  nothing nothing always something didnt dad tea...    A2  \n",
       "4  rick morgan dont know youre dont know hear may...    A2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dVeQAXj4rUj"
   },
   "source": [
    "**Проверяем таблицу на пропуски в столбце `text` и удаляем пустые строки.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:12:47.592092Z",
     "iopub.status.busy": "2023-08-15T18:12:47.591532Z",
     "iopub.status.idle": "2023-08-15T18:12:47.605309Z",
     "shell.execute_reply": "2023-08-15T18:12:47.604109Z",
     "shell.execute_reply.started": "2023-08-15T18:12:47.592048Z"
    },
    "id": "G97ECMcr4rUj",
    "outputId": "84c43d39-4aa7-4188-b69d-d5e5e86eafbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie               0\n",
       "text               11\n",
       "processed_text     11\n",
       "clean_text         11\n",
       "lemmatized_text    11\n",
       "level              50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REL4JVGi4rUk"
   },
   "source": [
    "**Удаляем пустые строки в столбце `text`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:12:56.769732Z",
     "iopub.status.busy": "2023-08-15T18:12:56.768520Z",
     "iopub.status.idle": "2023-08-15T18:12:56.778689Z",
     "shell.execute_reply": "2023-08-15T18:12:56.777318Z",
     "shell.execute_reply.started": "2023-08-15T18:12:56.769687Z"
    },
    "id": "8ikOHAzq4rUl"
   },
   "outputs": [],
   "source": [
    "df_1 = df_1.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T19:39:43.122539Z",
     "iopub.status.busy": "2023-08-02T19:39:43.122251Z",
     "iopub.status.idle": "2023-08-02T19:39:43.147621Z",
     "shell.execute_reply": "2023-08-02T19:39:43.146337Z",
     "shell.execute_reply.started": "2023-08-02T19:39:43.122515Z"
    },
    "id": "Vg7cl1HW4rUl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text level  \n",
       "0  little girl im policeman little girl dont afra...    A2  \n",
       "1  boy mom right luck tell theyre poison uh there...    A2  \n",
       "2  merle thats right hear bitch get problem bring...    A2  \n",
       "3  nothing nothing always something didnt dad tea...    A2  \n",
       "4  rick morgan dont know youre dont know hear may...    A2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:13:02.978383Z",
     "iopub.status.busy": "2023-08-15T18:13:02.977873Z",
     "iopub.status.idle": "2023-08-15T18:13:02.992132Z",
     "shell.execute_reply": "2023-08-15T18:13:02.989402Z",
     "shell.execute_reply.started": "2023-08-15T18:13:02.978345Z"
    },
    "id": "DHsNjSNg4rUl",
    "outputId": "aa098b90-57bb-4649-f9d7-cfbdbafb9b87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNZOzXcZ4rUm"
   },
   "source": [
    "**Исследуем столбец `level`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:13:45.561607Z",
     "iopub.status.busy": "2023-08-15T18:13:45.561139Z",
     "iopub.status.idle": "2023-08-15T18:13:45.620621Z",
     "shell.execute_reply": "2023-08-15T18:13:45.619237Z",
     "shell.execute_reply.started": "2023-08-15T18:13:45.561571Z"
    },
    "id": "YBh93F4S4rUm",
    "outputId": "239d2c75-179b-4f19-cca4-719e2ff5806f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SlingShot (2014) WEB.eng</td>\n",
       "      <td>Here's something that should hurt your brain. ...</td>\n",
       "      <td>heres something that should hurt your brain we...</td>\n",
       "      <td>heres something   hurt  brain   empty half    ...</td>\n",
       "      <td>here something hurt brain empty half bed hospi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Valentine`s.Day.2010.Subtitles.YIFY</td>\n",
       "      <td>&lt;i&gt;Hey there, all you sleepy Angelenos. A good...</td>\n",
       "      <td>hey there all you sleepy angelenos a good goo...</td>\n",
       "      <td>hey    sleepy angelenos  good good morning    ...</td>\n",
       "      <td>hey sleepy angelenos good good morning buddy r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Angela`s.Christmas.2018.WEBRip.Netflix</td>\n",
       "      <td>[match snaps, fizzles] [fire crackling] [puffs...</td>\n",
       "      <td>match snaps fizzles fire crackling puffs wagon...</td>\n",
       "      <td>match snaps fizzles fire crackling puffs wagon...</td>\n",
       "      <td>match snap fizzle fire crackle puff wagon whee...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Crown, The S01E01 - Wolferton Splash.en.SDH</td>\n",
       "      <td>[coughing] [coughing continues] [spits] [exhal...</td>\n",
       "      <td>coughing coughing continues spits exhales deep...</td>\n",
       "      <td>coughing coughing continues spits exhales deep...</td>\n",
       "      <td>cough cough continue spit exhales deeply toile...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Crown, The S01E01 - Wolferton Splash.en</td>\n",
       "      <td>In seeking his British nationalization, His Ro...</td>\n",
       "      <td>in seeking his british nationalization his roy...</td>\n",
       "      <td>seeking  british nationalization  royal highn...</td>\n",
       "      <td>seek british nationalization royal highness pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          movie  \\\n",
       "19                     SlingShot (2014) WEB.eng   \n",
       "22          Valentine`s.Day.2010.Subtitles.YIFY   \n",
       "23       Angela`s.Christmas.2018.WEBRip.Netflix   \n",
       "41  Crown, The S01E01 - Wolferton Splash.en.SDH   \n",
       "42      Crown, The S01E01 - Wolferton Splash.en   \n",
       "\n",
       "                                                 text  \\\n",
       "19  Here's something that should hurt your brain. ...   \n",
       "22  <i>Hey there, all you sleepy Angelenos. A good...   \n",
       "23  [match snaps, fizzles] [fire crackling] [puffs...   \n",
       "41  [coughing] [coughing continues] [spits] [exhal...   \n",
       "42  In seeking his British nationalization, His Ro...   \n",
       "\n",
       "                                       processed_text  \\\n",
       "19  heres something that should hurt your brain we...   \n",
       "22   hey there all you sleepy angelenos a good goo...   \n",
       "23  match snaps fizzles fire crackling puffs wagon...   \n",
       "41  coughing coughing continues spits exhales deep...   \n",
       "42  in seeking his british nationalization his roy...   \n",
       "\n",
       "                                           clean_text  \\\n",
       "19  heres something   hurt  brain   empty half    ...   \n",
       "22  hey    sleepy angelenos  good good morning    ...   \n",
       "23  match snaps fizzles fire crackling puffs wagon...   \n",
       "41  coughing coughing continues spits exhales deep...   \n",
       "42   seeking  british nationalization  royal highn...   \n",
       "\n",
       "                                      lemmatized_text level  \n",
       "19  here something hurt brain empty half bed hospi...   NaN  \n",
       "22  hey sleepy angelenos good good morning buddy r...   NaN  \n",
       "23  match snap fizzle fire crackle puff wagon whee...   NaN  \n",
       "41  cough cough continue spit exhales deeply toile...   NaN  \n",
       "42  seek british nationalization royal highness pr...   NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выводим строки с пропущенными значениями в столбце 'level'\n",
    "rows_with_missing_values = df_1[df_1['level'].isnull()]\n",
    "\n",
    "rows_with_missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RieCtuJw4rUm"
   },
   "source": [
    "**Заполняем пропуски в столбце `level`, для этого пишем функцию, которая заполняет пропущенное значение названием папки, которое и является значением категории сложности. Исключение составляют файлы, находящиеся в папке `Subtitles`, эти строки удаляем.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:27.974027Z",
     "iopub.status.busy": "2023-08-15T18:14:27.972185Z",
     "iopub.status.idle": "2023-08-15T18:14:28.025925Z",
     "shell.execute_reply": "2023-08-15T18:14:28.024485Z",
     "shell.execute_reply.started": "2023-08-15T18:14:27.973949Z"
    },
    "id": "Uh78tn0b4rUm",
    "outputId": "06d021fc-8c46-4684-ccd5-8834797bd3ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Walking Dead-S01E06-TS-19.English</td>\n",
       "      <td>- ( people yelling ) - ( radio chatter ) Hey h...</td>\n",
       "      <td>hey hey whoa whoa whoa whoa maam maam please ...</td>\n",
       "      <td>hey hey whoa whoa whoa whoa maam maam please p...</td>\n",
       "      <td>hey hey whoa whoa whoa whoa maam maam please p...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AmericanBeauty1999.BRRip</td>\n",
       "      <td>I need a father who's a role model, not some h...</td>\n",
       "      <td>i need a father whos a role model not some hor...</td>\n",
       "      <td>need  father whos  role model   horny geek bo...</td>\n",
       "      <td>need father whos role model horny geek boy who...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Angelas.Christmas.Wish.2020</td>\n",
       "      <td>Where are we going? Come on, Angela. But who'l...</td>\n",
       "      <td>where are we going come on angela but wholl mi...</td>\n",
       "      <td>going come  angela  wholl mind  sheep dada ...</td>\n",
       "      <td>go come angela wholl mind sheep dada want hear...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indiana Jones And The Last Crusade DVDRip Xvid...</td>\n",
       "      <td>Dismount! Herman's horse-sick! Chaps, no one w...</td>\n",
       "      <td>dismount hermans horsesick chaps no one wander...</td>\n",
       "      <td>dismount hermans horsesick chaps  one wander  ...</td>\n",
       "      <td>dismount herman horsesick chap one wander pass...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanic-resurrection_</td>\n",
       "      <td>Mr. Santos, so good to see you. We saved your ...</td>\n",
       "      <td>mr santos so good to see you we saved your usu...</td>\n",
       "      <td>mr santos  good  see   saved  usual table mr s...</td>\n",
       "      <td>mr santos good see save usual table mr santos ...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Men.In.Black.1997.720p.Bluray.x264-SEPTiC</td>\n",
       "      <td>Goddamn bugs. Oh, shit! Oh, crap. Well, Nick t...</td>\n",
       "      <td>goddamn bugs oh shit oh crap well nick the dic...</td>\n",
       "      <td>goddamn bugs oh shit oh crap well nick  dick  ...</td>\n",
       "      <td>goddamn bug oh shit oh crap well nick dick sur...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rat.Race.2001.1080p.WEB-DL.DD5.1.H264-FGT</td>\n",
       "      <td>Well, I wake up in the mornin' Each and every ...</td>\n",
       "      <td>well i wake up in the mornin each and every da...</td>\n",
       "      <td>well  wake    mornin   every day   sit    tabl...</td>\n",
       "      <td>well wake mornin every day sit table hear dadd...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E01.2160p.BluRay.Re...</td>\n",
       "      <td>200 million years ago, our planet looked very ...</td>\n",
       "      <td>million years ago our planet looked very diff...</td>\n",
       "      <td>million years ago  planet looked  different   ...</td>\n",
       "      <td>million year ago planet look different today e...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E02.2160p.BluRay.Re...</td>\n",
       "      <td>Asia - the largest of all the Earth's continen...</td>\n",
       "      <td>asia the largest of all the earths continents ...</td>\n",
       "      <td>asia  largest    earths continents  stretches ...</td>\n",
       "      <td>asia large earths continent stretch equator be...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E03.2160p.BluRay.Re...</td>\n",
       "      <td>At the southern tip of South America, the Ande...</td>\n",
       "      <td>at the southern tip of south america the andes...</td>\n",
       "      <td>southern tip  south america  andes mountains...</td>\n",
       "      <td>southern tip south america andes mountain rise...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E04.2160p.BluRay.Re...</td>\n",
       "      <td>Australia - an island continent cast adrift du...</td>\n",
       "      <td>australia an island continent cast adrift duri...</td>\n",
       "      <td>australia  island continent cast adrift   time...</td>\n",
       "      <td>australia island continent cast adrift time di...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E05.2160p.BluRay.Re...</td>\n",
       "      <td>Europe. Home to 850 million people. This is a ...</td>\n",
       "      <td>europe home to million people this is a contin...</td>\n",
       "      <td>europe home  million people    continent    tr...</td>\n",
       "      <td>europe home million people continent transform...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E06.2160p.BluRay.Re...</td>\n",
       "      <td>One continent on our planet changes more drama...</td>\n",
       "      <td>one continent on our planet changes more drama...</td>\n",
       "      <td>one continent   planet changes  dramatically  ...</td>\n",
       "      <td>one continent planet change dramatically north...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Seven.Worlds.One.Planet.S01E07.2160p.BluRay.Re...</td>\n",
       "      <td>Africa. No continent on Earth today has such s...</td>\n",
       "      <td>africa no continent on earth today has such sp...</td>\n",
       "      <td>africa  continent  earth today   spectacular w...</td>\n",
       "      <td>africa continent earth today spectacular wildl...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SlingShot (2014) WEB.eng</td>\n",
       "      <td>Here's something that should hurt your brain. ...</td>\n",
       "      <td>heres something that should hurt your brain we...</td>\n",
       "      <td>heres something   hurt  brain   empty half    ...</td>\n",
       "      <td>here something hurt brain empty half bed hospi...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                movie  \\\n",
       "0       The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1                The Walking Dead-S01E02-Guts.English   \n",
       "2   The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3               The Walking Dead-S01E04-Vatos.English   \n",
       "4            The Walking Dead-S01E05-Wildfire.English   \n",
       "5               The Walking Dead-S01E06-TS-19.English   \n",
       "6                            AmericanBeauty1999.BRRip   \n",
       "7                         Angelas.Christmas.Wish.2020   \n",
       "8   Indiana Jones And The Last Crusade DVDRip Xvid...   \n",
       "9                              mechanic-resurrection_   \n",
       "10          Men.In.Black.1997.720p.Bluray.x264-SEPTiC   \n",
       "11          Rat.Race.2001.1080p.WEB-DL.DD5.1.H264-FGT   \n",
       "12  Seven.Worlds.One.Planet.S01E01.2160p.BluRay.Re...   \n",
       "13  Seven.Worlds.One.Planet.S01E02.2160p.BluRay.Re...   \n",
       "14  Seven.Worlds.One.Planet.S01E03.2160p.BluRay.Re...   \n",
       "15  Seven.Worlds.One.Planet.S01E04.2160p.BluRay.Re...   \n",
       "16  Seven.Worlds.One.Planet.S01E05.2160p.BluRay.Re...   \n",
       "17  Seven.Worlds.One.Planet.S01E06.2160p.BluRay.Re...   \n",
       "18  Seven.Worlds.One.Planet.S01E07.2160p.BluRay.Re...   \n",
       "19                           SlingShot (2014) WEB.eng   \n",
       "\n",
       "                                                 text  \\\n",
       "0   ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1   - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2   ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3   ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4   - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "5   - ( people yelling ) - ( radio chatter ) Hey h...   \n",
       "6   I need a father who's a role model, not some h...   \n",
       "7   Where are we going? Come on, Angela. But who'l...   \n",
       "8   Dismount! Herman's horse-sick! Chaps, no one w...   \n",
       "9   Mr. Santos, so good to see you. We saved your ...   \n",
       "10  Goddamn bugs. Oh, shit! Oh, crap. Well, Nick t...   \n",
       "11  Well, I wake up in the mornin' Each and every ...   \n",
       "12  200 million years ago, our planet looked very ...   \n",
       "13  Asia - the largest of all the Earth's continen...   \n",
       "14  At the southern tip of South America, the Ande...   \n",
       "15  Australia - an island continent cast adrift du...   \n",
       "16  Europe. Home to 850 million people. This is a ...   \n",
       "17  One continent on our planet changes more drama...   \n",
       "18  Africa. No continent on Earth today has such s...   \n",
       "19  Here's something that should hurt your brain. ...   \n",
       "\n",
       "                                       processed_text  \\\n",
       "0    little girl im a policeman little girl dont b...   \n",
       "1    boy mom right here any luck how do we tell if...   \n",
       "2    merle thats right you heard me bitch you got ...   \n",
       "3    what nothing its not nothing its always somet...   \n",
       "4    rick morgan i dont know if youre out there i ...   \n",
       "5    hey hey whoa whoa whoa whoa maam maam please ...   \n",
       "6   i need a father whos a role model not some hor...   \n",
       "7   where are we going come on angela but wholl mi...   \n",
       "8   dismount hermans horsesick chaps no one wander...   \n",
       "9   mr santos so good to see you we saved your usu...   \n",
       "10  goddamn bugs oh shit oh crap well nick the dic...   \n",
       "11  well i wake up in the mornin each and every da...   \n",
       "12   million years ago our planet looked very diff...   \n",
       "13  asia the largest of all the earths continents ...   \n",
       "14  at the southern tip of south america the andes...   \n",
       "15  australia an island continent cast adrift duri...   \n",
       "16  europe home to million people this is a contin...   \n",
       "17  one continent on our planet changes more drama...   \n",
       "18  africa no continent on earth today has such sp...   \n",
       "19  heres something that should hurt your brain we...   \n",
       "\n",
       "                                           clean_text  \\\n",
       "0   little girl im  policeman little girl dont  af...   \n",
       "1   boy mom right   luck    tell  theyre poison uh...   \n",
       "2   merle thats right  heard  bitch  got  problem ...   \n",
       "3    nothing   nothing  always something didnt dad...   \n",
       "4   rick morgan  dont know  youre    dont know    ...   \n",
       "5   hey hey whoa whoa whoa whoa maam maam please p...   \n",
       "6    need  father whos  role model   horny geek bo...   \n",
       "7      going come  angela  wholl mind  sheep dada ...   \n",
       "8   dismount hermans horsesick chaps  one wander  ...   \n",
       "9   mr santos  good  see   saved  usual table mr s...   \n",
       "10  goddamn bugs oh shit oh crap well nick  dick  ...   \n",
       "11  well  wake    mornin   every day   sit    tabl...   \n",
       "12  million years ago  planet looked  different   ...   \n",
       "13  asia  largest    earths continents  stretches ...   \n",
       "14    southern tip  south america  andes mountains...   \n",
       "15  australia  island continent cast adrift   time...   \n",
       "16  europe home  million people    continent    tr...   \n",
       "17  one continent   planet changes  dramatically  ...   \n",
       "18  africa  continent  earth today   spectacular w...   \n",
       "19  heres something   hurt  brain   empty half    ...   \n",
       "\n",
       "                                      lemmatized_text level  \n",
       "0   little girl im policeman little girl dont afra...    A2  \n",
       "1   boy mom right luck tell theyre poison uh there...    A2  \n",
       "2   merle thats right hear bitch get problem bring...    A2  \n",
       "3   nothing nothing always something didnt dad tea...    A2  \n",
       "4   rick morgan dont know youre dont know hear may...    A2  \n",
       "5   hey hey whoa whoa whoa whoa maam maam please p...    A2  \n",
       "6   need father whos role model horny geek boy who...    B1  \n",
       "7   go come angela wholl mind sheep dada want hear...    B1  \n",
       "8   dismount herman horsesick chap one wander pass...    B1  \n",
       "9   mr santos good see save usual table mr santos ...    B1  \n",
       "10  goddamn bug oh shit oh crap well nick dick sur...    B1  \n",
       "11  well wake mornin every day sit table hear dadd...    B1  \n",
       "12  million year ago planet look different today e...    B1  \n",
       "13  asia large earths continent stretch equator be...    B1  \n",
       "14  southern tip south america andes mountain rise...    B1  \n",
       "15  australia island continent cast adrift time di...    B1  \n",
       "16  europe home million people continent transform...    B1  \n",
       "17  one continent planet change dramatically north...    B1  \n",
       "18  africa continent earth today spectacular wildl...    B1  \n",
       "19  here something hurt brain empty half bed hospi...    B1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задаем путь к базовой папке с файлами SRT\n",
    "base_folder = 'D:/Яндекс Практикум/17. Мастерская_2/Датасет/Subtitles_all'\n",
    "\n",
    "# Функция для получения названия папки из полного пути к файлу\n",
    "def get_folder_name(file_path):\n",
    "    folder_path = os.path.dirname(file_path)\n",
    "    return os.path.basename(folder_path)\n",
    "\n",
    "# Создаем словарь с соответствием названий файлов без расширения и папок\n",
    "filename_to_folder = {}\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Проверяем, что это директория\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if file_path.endswith(\".srt\"):  # Проверяем, что это файл SRT\n",
    "                filename_without_extension = os.path.splitext(filename)[0]\n",
    "                filename_to_folder[filename_without_extension] = folder_name\n",
    "\n",
    "# Заполняем пропуски в столбце 'level' соответствующим значением из словаря\n",
    "df_1['level'] = df_1['level'].fillna(df_1['movie'].apply(lambda x: filename_to_folder.get(x, '')))\n",
    "\n",
    "# Выводим первые несколько строк для проверки\n",
    "df_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:34.812221Z",
     "iopub.status.busy": "2023-08-15T18:14:34.811706Z",
     "iopub.status.idle": "2023-08-15T18:14:34.825013Z",
     "shell.execute_reply": "2023-08-15T18:14:34.823756Z",
     "shell.execute_reply.started": "2023-08-15T18:14:34.812185Z"
    },
    "id": "17nEN2Ia4rUn",
    "outputId": "3b043833-5c05-4a0c-8403-a10e10769b74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie              0\n",
       "text               0\n",
       "processed_text     0\n",
       "clean_text         0\n",
       "lemmatized_text    0\n",
       "level              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:38.299178Z",
     "iopub.status.busy": "2023-08-15T18:14:38.298675Z",
     "iopub.status.idle": "2023-08-15T18:14:38.311347Z",
     "shell.execute_reply": "2023-08-15T18:14:38.309860Z",
     "shell.execute_reply.started": "2023-08-15T18:14:38.299145Z"
    },
    "id": "2kM_a7Pt4rUn",
    "outputId": "fb992228-7235-44a3-93c5-6c9edaaf04e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Подсчитываем уникальные значения в столбце 'level' и их количество\n",
    "level_counts = df_1['level'].value_counts()\n",
    "\n",
    "# Выводим количества строк со значением 'subtitles' в столбце 'level'\n",
    "print(level_counts['Subtitles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:44.162661Z",
     "iopub.status.busy": "2023-08-15T18:14:44.162181Z",
     "iopub.status.idle": "2023-08-15T18:14:44.185892Z",
     "shell.execute_reply": "2023-08-15T18:14:44.184832Z",
     "shell.execute_reply.started": "2023-08-15T18:14:44.162623Z"
    },
    "id": "O_7Pzv9o4rUn",
    "outputId": "c2d64af0-f85f-4ab7-a106-6bf829108b14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>BrenВ.Brown.The.Call.to.Courage.2019.720.NF.72...</td>\n",
       "      <td>[presenter] &lt;i&gt;She spent 20 years studying cou...</td>\n",
       "      <td>presenter she spent years studying courage vul...</td>\n",
       "      <td>presenter  spent years studying courage vulner...</td>\n",
       "      <td>presenter spend year study courage vulnerabili...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Casper</td>\n",
       "      <td>[Music] [Music] okay one picture in my history...</td>\n",
       "      <td>music music okay one picture in my history thi...</td>\n",
       "      <td>music music okay one picture   history   im af...</td>\n",
       "      <td>music music okay one picture history im afraid...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Gogo_Loves_English</td>\n",
       "      <td>[Music] so [Music] wow [Music] hello hello my ...</td>\n",
       "      <td>music so music wow music hello hello my names ...</td>\n",
       "      <td>music  music wow music hello hello  names tony...</td>\n",
       "      <td>music music wow music hello hello name tony wh...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Harry_Potter_and_the_philosophers_stone(2001)</td>\n",
       "      <td>I should've known that you would be here, Prof...</td>\n",
       "      <td>i shouldve known that you would be here profes...</td>\n",
       "      <td>shouldve known   would   professor mcgonagall...</td>\n",
       "      <td>shouldve know would professor mcgonagall good ...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Pride_and_Prejudice</td>\n",
       "      <td>[Music] so [Music] um [Music] [Music] so [Musi...</td>\n",
       "      <td>music so music um music music so music lydia k...</td>\n",
       "      <td>music  music um music music  music lydia kitty...</td>\n",
       "      <td>music music um music music music lydia kitty m...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>The_Ghost_Writer</td>\n",
       "      <td>[Music] [Music] so [Music] [Music] [Music] [Mu...</td>\n",
       "      <td>music music so music music music music applaus...</td>\n",
       "      <td>music music  music music music music applause ...</td>\n",
       "      <td>music music music music music music applause h...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Westworld_scenes_of_Dr_Robert_Ford</td>\n",
       "      <td>[Music] no one's complained [Music] there's th...</td>\n",
       "      <td>music no ones complained music theres the lady...</td>\n",
       "      <td>music  ones complained music theres  lady   wh...</td>\n",
       "      <td>music one complain music there lady white shoe...</td>\n",
       "      <td>Subtitles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 movie  \\\n",
       "178  BrenВ.Brown.The.Call.to.Courage.2019.720.NF.72...   \n",
       "182                                             Casper   \n",
       "200                                 Gogo_Loves_English   \n",
       "203      Harry_Potter_and_the_philosophers_stone(2001)   \n",
       "235                                Pride_and_Prejudice   \n",
       "249                                   The_Ghost_Writer   \n",
       "276                 Westworld_scenes_of_Dr_Robert_Ford   \n",
       "\n",
       "                                                  text  \\\n",
       "178  [presenter] <i>She spent 20 years studying cou...   \n",
       "182  [Music] [Music] okay one picture in my history...   \n",
       "200  [Music] so [Music] wow [Music] hello hello my ...   \n",
       "203  I should've known that you would be here, Prof...   \n",
       "235  [Music] so [Music] um [Music] [Music] so [Musi...   \n",
       "249  [Music] [Music] so [Music] [Music] [Music] [Mu...   \n",
       "276  [Music] no one's complained [Music] there's th...   \n",
       "\n",
       "                                        processed_text  \\\n",
       "178  presenter she spent years studying courage vul...   \n",
       "182  music music okay one picture in my history thi...   \n",
       "200  music so music wow music hello hello my names ...   \n",
       "203  i shouldve known that you would be here profes...   \n",
       "235  music so music um music music so music lydia k...   \n",
       "249  music music so music music music music applaus...   \n",
       "276  music no ones complained music theres the lady...   \n",
       "\n",
       "                                            clean_text  \\\n",
       "178  presenter  spent years studying courage vulner...   \n",
       "182  music music okay one picture   history   im af...   \n",
       "200  music  music wow music hello hello  names tony...   \n",
       "203   shouldve known   would   professor mcgonagall...   \n",
       "235  music  music um music music  music lydia kitty...   \n",
       "249  music music  music music music music applause ...   \n",
       "276  music  ones complained music theres  lady   wh...   \n",
       "\n",
       "                                       lemmatized_text      level  \n",
       "178  presenter spend year study courage vulnerabili...  Subtitles  \n",
       "182  music music okay one picture history im afraid...  Subtitles  \n",
       "200  music music wow music hello hello name tony wh...  Subtitles  \n",
       "203  shouldve know would professor mcgonagall good ...  Subtitles  \n",
       "235  music music um music music music lydia kitty m...  Subtitles  \n",
       "249  music music music music music music applause h...  Subtitles  \n",
       "276  music one complain music there lady white shoe...  Subtitles  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df_1[df_1['level'] == 'Subtitles']\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:46.958077Z",
     "iopub.status.busy": "2023-08-15T18:14:46.957016Z",
     "iopub.status.idle": "2023-08-15T18:14:46.964630Z",
     "shell.execute_reply": "2023-08-15T18:14:46.963290Z",
     "shell.execute_reply.started": "2023-08-15T18:14:46.958036Z"
    },
    "id": "06Jg4i8G4rUo"
   },
   "outputs": [],
   "source": [
    "df_1 = df_1[df_1.level != 'Subtitles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:50.171222Z",
     "iopub.status.busy": "2023-08-15T18:14:50.170765Z",
     "iopub.status.idle": "2023-08-15T18:14:50.178068Z",
     "shell.execute_reply": "2023-08-15T18:14:50.176543Z",
     "shell.execute_reply.started": "2023-08-15T18:14:50.171188Z"
    },
    "id": "5TV3ahwP4rUo"
   },
   "outputs": [],
   "source": [
    "df_1 = df_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:52.332997Z",
     "iopub.status.busy": "2023-08-15T18:14:52.332593Z",
     "iopub.status.idle": "2023-08-15T18:14:52.358306Z",
     "shell.execute_reply": "2023-08-15T18:14:52.357331Z",
     "shell.execute_reply.started": "2023-08-15T18:14:52.332966Z"
    },
    "id": "0Hd6r-RM4rUo",
    "outputId": "80215a47-3917-4432-ef65-da1665b216e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text level  \n",
       "0  little girl im policeman little girl dont afra...    A2  \n",
       "1  boy mom right luck tell theyre poison uh there...    A2  \n",
       "2  merle thats right hear bitch get problem bring...    A2  \n",
       "3  nothing nothing always something didnt dad tea...    A2  \n",
       "4  rick morgan dont know youre dont know hear may...    A2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRQStISy4rUp"
   },
   "source": [
    "**Проверяем, что в таблице остались только заполненные строки, столбец `level` содержит корректные значения.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:14:58.302465Z",
     "iopub.status.busy": "2023-08-15T18:14:58.301583Z",
     "iopub.status.idle": "2023-08-15T18:14:58.313382Z",
     "shell.execute_reply": "2023-08-15T18:14:58.311904Z",
     "shell.execute_reply.started": "2023-08-15T18:14:58.302417Z"
    },
    "id": "wtA8VmFN4rUp",
    "outputId": "eb8c1da7-4d3a-4d1f-9900-c9cd2570736e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A2', 'B1', 'B2', 'C1'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuVS7tz-4rUq"
   },
   "source": [
    "### Создаём новые признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww7Loo8R4rUq"
   },
   "source": [
    "**Для создания новых признаков считаем вхождение слов каждой категории в текст субтитров, количество слов в тексте, а также процент слов кждой категории в тексте.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:15:25.871183Z",
     "iopub.status.busy": "2023-08-15T18:15:25.870033Z",
     "iopub.status.idle": "2023-08-15T18:16:32.019982Z",
     "shell.execute_reply": "2023-08-15T18:16:32.018551Z",
     "shell.execute_reply.started": "2023-08-15T18:15:25.871134Z"
    },
    "id": "NS-HkBir4rUq",
    "outputId": "4b6a417c-8c4a-480e-9314-8f510fec1785"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "      <th>A2_word_count</th>\n",
       "      <th>B1_word_count</th>\n",
       "      <th>B2_word_count</th>\n",
       "      <th>C1_word_count</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>A2_word_%</th>\n",
       "      <th>B1_word_%</th>\n",
       "      <th>B2_word_%</th>\n",
       "      <th>C1_word_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "      <td>261</td>\n",
       "      <td>152</td>\n",
       "      <td>142</td>\n",
       "      <td>65</td>\n",
       "      <td>1676</td>\n",
       "      <td>0.155728</td>\n",
       "      <td>0.090692</td>\n",
       "      <td>0.084726</td>\n",
       "      <td>0.038783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "      <td>246</td>\n",
       "      <td>163</td>\n",
       "      <td>156</td>\n",
       "      <td>39</td>\n",
       "      <td>1686</td>\n",
       "      <td>0.145907</td>\n",
       "      <td>0.096679</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.023132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "      <td>368</td>\n",
       "      <td>151</td>\n",
       "      <td>181</td>\n",
       "      <td>74</td>\n",
       "      <td>2170</td>\n",
       "      <td>0.169585</td>\n",
       "      <td>0.069585</td>\n",
       "      <td>0.083410</td>\n",
       "      <td>0.034101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "      <td>328</td>\n",
       "      <td>188</td>\n",
       "      <td>169</td>\n",
       "      <td>46</td>\n",
       "      <td>1932</td>\n",
       "      <td>0.169772</td>\n",
       "      <td>0.097308</td>\n",
       "      <td>0.087474</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "      <td>268</td>\n",
       "      <td>170</td>\n",
       "      <td>129</td>\n",
       "      <td>57</td>\n",
       "      <td>1597</td>\n",
       "      <td>0.167815</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.080776</td>\n",
       "      <td>0.035692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text level  A2_word_count  \\\n",
       "0  little girl im policeman little girl dont afra...    A2            261   \n",
       "1  boy mom right luck tell theyre poison uh there...    A2            246   \n",
       "2  merle thats right hear bitch get problem bring...    A2            368   \n",
       "3  nothing nothing always something didnt dad tea...    A2            328   \n",
       "4  rick morgan dont know youre dont know hear may...    A2            268   \n",
       "\n",
       "   B1_word_count  B2_word_count  C1_word_count  total_word_count  A2_word_%  \\\n",
       "0            152            142             65              1676   0.155728   \n",
       "1            163            156             39              1686   0.145907   \n",
       "2            151            181             74              2170   0.169585   \n",
       "3            188            169             46              1932   0.169772   \n",
       "4            170            129             57              1597   0.167815   \n",
       "\n",
       "   B1_word_%  B2_word_%  C1_word_%  \n",
       "0   0.090692   0.084726   0.038783  \n",
       "1   0.096679   0.092527   0.023132  \n",
       "2   0.069585   0.083410   0.034101  \n",
       "3   0.097308   0.087474   0.023810  \n",
       "4   0.106450   0.080776   0.035692  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаем функцию для подсчета количества вхождений слов в текст\n",
    "def count_words(text, word_list):\n",
    "    word_count = sum(1 for word in text.split() if word in word_list)\n",
    "    return word_count\n",
    "\n",
    "# Применяем функцию для каждого текста и каждого списка слов\n",
    "df_1['A2_word_count'] = df_1['lemmatized_text'].apply(lambda x: count_words(x, A2_words))\n",
    "df_1['B1_word_count'] = df_1['lemmatized_text'].apply(lambda x: count_words(x, B1_words))\n",
    "df_1['B2_word_count'] = df_1['lemmatized_text'].apply(lambda x: count_words(x, B2_words))\n",
    "df_1['C1_word_count'] = df_1['lemmatized_text'].apply(lambda x: count_words(x, C1_words))\n",
    "\n",
    "# Добавляем столбец с общим количеством слов в тексте\n",
    "#df_1['total_word_count'] = df_1['lemmatized_text'].apply(lambda x: len(x))\n",
    "df_1['total_word_count'] = df_1['lemmatized_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Добавляем столбцы с долей слов каждой категории в тексте\n",
    "df_1['A2_word_%'] = df_1['A2_word_count'] / df_1['total_word_count']\n",
    "df_1['B1_word_%'] = df_1['B1_word_count'] / df_1['total_word_count']\n",
    "df_1['B2_word_%'] = df_1['B2_word_count'] / df_1['total_word_count']\n",
    "df_1['C1_word_%'] = df_1['C1_word_count'] / df_1['total_word_count']\n",
    "\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHRAK5Rp4rUr"
   },
   "source": [
    "**Масштабируем все числовые признаки.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:17:57.784378Z",
     "iopub.status.busy": "2023-08-15T18:17:57.783814Z",
     "iopub.status.idle": "2023-08-15T18:17:57.791593Z",
     "shell.execute_reply": "2023-08-15T18:17:57.789809Z",
     "shell.execute_reply.started": "2023-08-15T18:17:57.784338Z"
    },
    "id": "Q5a8o-_64rUr"
   },
   "outputs": [],
   "source": [
    "numeric = ['A2_word_count', 'B1_word_count', 'B2_word_count',\n",
    "           'C1_word_count', 'total_word_count', 'A2_word_%', 'B1_word_%', 'B2_word_%', 'C1_word_%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:18:18.237490Z",
     "iopub.status.busy": "2023-08-15T18:18:18.236986Z",
     "iopub.status.idle": "2023-08-15T18:18:18.259383Z",
     "shell.execute_reply": "2023-08-15T18:18:18.258342Z",
     "shell.execute_reply.started": "2023-08-15T18:18:18.237450Z"
    },
    "id": "w7o52JuX4rUr",
    "outputId": "2bedef7c-f01a-4ac2-c2e9-97196350c8d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_1[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:18:36.515249Z",
     "iopub.status.busy": "2023-08-15T18:18:36.514822Z",
     "iopub.status.idle": "2023-08-15T18:18:36.559084Z",
     "shell.execute_reply": "2023-08-15T18:18:36.557453Z",
     "shell.execute_reply.started": "2023-08-15T18:18:36.515217Z"
    },
    "id": "3QnV6E954rUr",
    "outputId": "4ef0685c-3185-454b-d218-ab88e6e46478"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>level</th>\n",
       "      <th>A2_word_count</th>\n",
       "      <th>B1_word_count</th>\n",
       "      <th>B2_word_count</th>\n",
       "      <th>C1_word_count</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>A2_word_%</th>\n",
       "      <th>B1_word_%</th>\n",
       "      <th>B2_word_%</th>\n",
       "      <th>C1_word_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>( bugs chittering ) ( brakes squeak ) - ( engi...</td>\n",
       "      <td>little girl im a policeman little girl dont b...</td>\n",
       "      <td>little girl im  policeman little girl dont  af...</td>\n",
       "      <td>little girl im policeman little girl dont afra...</td>\n",
       "      <td>A2</td>\n",
       "      <td>-1.351579</td>\n",
       "      <td>-1.393482</td>\n",
       "      <td>-1.389824</td>\n",
       "      <td>-1.241052</td>\n",
       "      <td>-1.289059</td>\n",
       "      <td>-0.427852</td>\n",
       "      <td>-0.483181</td>\n",
       "      <td>-0.175686</td>\n",
       "      <td>0.366513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>- ( birds chirping ) - ( bugs chittering ) Boy...</td>\n",
       "      <td>boy mom right here any luck how do we tell if...</td>\n",
       "      <td>boy mom right   luck    tell  theyre poison uh...</td>\n",
       "      <td>boy mom right luck tell theyre poison uh there...</td>\n",
       "      <td>A2</td>\n",
       "      <td>-1.410225</td>\n",
       "      <td>-1.320338</td>\n",
       "      <td>-1.279023</td>\n",
       "      <td>-1.721184</td>\n",
       "      <td>-1.282792</td>\n",
       "      <td>-0.886396</td>\n",
       "      <td>-0.084365</td>\n",
       "      <td>0.295649</td>\n",
       "      <td>-1.584313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>( thunder rumbling ) Merle: That's right. You ...</td>\n",
       "      <td>merle thats right you heard me bitch you got ...</td>\n",
       "      <td>merle thats right  heard  bitch  got  problem ...</td>\n",
       "      <td>merle thats right hear bitch get problem bring...</td>\n",
       "      <td>A2</td>\n",
       "      <td>-0.933233</td>\n",
       "      <td>-1.400132</td>\n",
       "      <td>-1.081164</td>\n",
       "      <td>-1.074852</td>\n",
       "      <td>-0.979498</td>\n",
       "      <td>0.219185</td>\n",
       "      <td>-1.889329</td>\n",
       "      <td>-0.255160</td>\n",
       "      <td>-0.217001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>( birds chirping ) - What? - Nothing. It's not...</td>\n",
       "      <td>what nothing its not nothing its always somet...</td>\n",
       "      <td>nothing   nothing  always something didnt dad...</td>\n",
       "      <td>nothing nothing always something didnt dad tea...</td>\n",
       "      <td>A2</td>\n",
       "      <td>-1.089624</td>\n",
       "      <td>-1.154103</td>\n",
       "      <td>-1.176136</td>\n",
       "      <td>-1.591918</td>\n",
       "      <td>-1.128639</td>\n",
       "      <td>0.227916</td>\n",
       "      <td>-0.042397</td>\n",
       "      <td>-0.009620</td>\n",
       "      <td>-1.499822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>- ( walkie-talkie squawks ) - Rick: Morgan, I ...</td>\n",
       "      <td>rick morgan i dont know if youre out there i ...</td>\n",
       "      <td>rick morgan  dont know  youre    dont know    ...</td>\n",
       "      <td>rick morgan dont know youre dont know hear may...</td>\n",
       "      <td>A2</td>\n",
       "      <td>-1.324210</td>\n",
       "      <td>-1.273792</td>\n",
       "      <td>-1.492710</td>\n",
       "      <td>-1.388785</td>\n",
       "      <td>-1.338563</td>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.566587</td>\n",
       "      <td>-0.414284</td>\n",
       "      <td>-0.018749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                                text  \\\n",
       "0  ( bugs chittering ) ( brakes squeak ) - ( engi...   \n",
       "1  - ( birds chirping ) - ( bugs chittering ) Boy...   \n",
       "2  ( thunder rumbling ) Merle: That's right. You ...   \n",
       "3  ( birds chirping ) - What? - Nothing. It's not...   \n",
       "4  - ( walkie-talkie squawks ) - Rick: Morgan, I ...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0   little girl im a policeman little girl dont b...   \n",
       "1   boy mom right here any luck how do we tell if...   \n",
       "2   merle thats right you heard me bitch you got ...   \n",
       "3   what nothing its not nothing its always somet...   \n",
       "4   rick morgan i dont know if youre out there i ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  little girl im  policeman little girl dont  af...   \n",
       "1  boy mom right   luck    tell  theyre poison uh...   \n",
       "2  merle thats right  heard  bitch  got  problem ...   \n",
       "3   nothing   nothing  always something didnt dad...   \n",
       "4  rick morgan  dont know  youre    dont know    ...   \n",
       "\n",
       "                                     lemmatized_text level  A2_word_count  \\\n",
       "0  little girl im policeman little girl dont afra...    A2      -1.351579   \n",
       "1  boy mom right luck tell theyre poison uh there...    A2      -1.410225   \n",
       "2  merle thats right hear bitch get problem bring...    A2      -0.933233   \n",
       "3  nothing nothing always something didnt dad tea...    A2      -1.089624   \n",
       "4  rick morgan dont know youre dont know hear may...    A2      -1.324210   \n",
       "\n",
       "   B1_word_count  B2_word_count  C1_word_count  total_word_count  A2_word_%  \\\n",
       "0      -1.393482      -1.389824      -1.241052         -1.289059  -0.427852   \n",
       "1      -1.320338      -1.279023      -1.721184         -1.282792  -0.886396   \n",
       "2      -1.400132      -1.081164      -1.074852         -0.979498   0.219185   \n",
       "3      -1.154103      -1.176136      -1.591918         -1.128639   0.227916   \n",
       "4      -1.273792      -1.492710      -1.388785         -1.338563   0.136510   \n",
       "\n",
       "   B1_word_%  B2_word_%  C1_word_%  \n",
       "0  -0.483181  -0.175686   0.366513  \n",
       "1  -0.084365   0.295649  -1.584313  \n",
       "2  -1.889329  -0.255160  -0.217001  \n",
       "3  -0.042397  -0.009620  -1.499822  \n",
       "4   0.566587  -0.414284  -0.018749  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df_1[numeric] = scaler.transform(df_1[numeric])\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:18:42.900570Z",
     "iopub.status.busy": "2023-08-15T18:18:42.899959Z",
     "iopub.status.idle": "2023-08-15T18:18:42.930130Z",
     "shell.execute_reply": "2023-08-15T18:18:42.928444Z",
     "shell.execute_reply.started": "2023-08-15T18:18:42.900524Z"
    },
    "id": "ZUUwlFCz4rUs",
    "outputId": "73e7cc9c-9ff8-4849-ff1f-effc1d5b32ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2_word_count</th>\n",
       "      <th>B1_word_count</th>\n",
       "      <th>B2_word_count</th>\n",
       "      <th>C1_word_count</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>A2_word_%</th>\n",
       "      <th>B1_word_%</th>\n",
       "      <th>B2_word_%</th>\n",
       "      <th>C1_word_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A2_word_count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945672</td>\n",
       "      <td>0.923453</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.966391</td>\n",
       "      <td>-0.072345</td>\n",
       "      <td>-0.069939</td>\n",
       "      <td>-0.249865</td>\n",
       "      <td>-0.096646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1_word_count</th>\n",
       "      <td>0.945672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931060</td>\n",
       "      <td>0.849370</td>\n",
       "      <td>0.934342</td>\n",
       "      <td>-0.144309</td>\n",
       "      <td>0.151579</td>\n",
       "      <td>-0.183528</td>\n",
       "      <td>-0.058388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2_word_count</th>\n",
       "      <td>0.923453</td>\n",
       "      <td>0.931060</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836127</td>\n",
       "      <td>0.893735</td>\n",
       "      <td>-0.096822</td>\n",
       "      <td>0.063359</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>-0.006543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1_word_count</th>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.849370</td>\n",
       "      <td>0.836127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887569</td>\n",
       "      <td>-0.217707</td>\n",
       "      <td>-0.073763</td>\n",
       "      <td>-0.232509</td>\n",
       "      <td>0.285164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_word_count</th>\n",
       "      <td>0.966391</td>\n",
       "      <td>0.934342</td>\n",
       "      <td>0.893735</td>\n",
       "      <td>0.887569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.259290</td>\n",
       "      <td>-0.170055</td>\n",
       "      <td>-0.361024</td>\n",
       "      <td>-0.124797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2_word_%</th>\n",
       "      <td>-0.072345</td>\n",
       "      <td>-0.144309</td>\n",
       "      <td>-0.096822</td>\n",
       "      <td>-0.217707</td>\n",
       "      <td>-0.259290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407702</td>\n",
       "      <td>0.549889</td>\n",
       "      <td>0.082971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1_word_%</th>\n",
       "      <td>-0.069939</td>\n",
       "      <td>0.151579</td>\n",
       "      <td>0.063359</td>\n",
       "      <td>-0.073763</td>\n",
       "      <td>-0.170055</td>\n",
       "      <td>0.407702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.319421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2_word_%</th>\n",
       "      <td>-0.249865</td>\n",
       "      <td>-0.183528</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>-0.232509</td>\n",
       "      <td>-0.361024</td>\n",
       "      <td>0.549889</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1_word_%</th>\n",
       "      <td>-0.096646</td>\n",
       "      <td>-0.058388</td>\n",
       "      <td>-0.006543</td>\n",
       "      <td>0.285164</td>\n",
       "      <td>-0.124797</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.319421</td>\n",
       "      <td>0.266357</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A2_word_count  B1_word_count  B2_word_count  C1_word_count  \\\n",
       "A2_word_count          1.000000       0.945672       0.923453       0.867351   \n",
       "B1_word_count          0.945672       1.000000       0.931060       0.849370   \n",
       "B2_word_count          0.923453       0.931060       1.000000       0.836127   \n",
       "C1_word_count          0.867351       0.849370       0.836127       1.000000   \n",
       "total_word_count       0.966391       0.934342       0.893735       0.887569   \n",
       "A2_word_%             -0.072345      -0.144309      -0.096822      -0.217707   \n",
       "B1_word_%             -0.069939       0.151579       0.063359      -0.073763   \n",
       "B2_word_%             -0.249865      -0.183528       0.023103      -0.232509   \n",
       "C1_word_%             -0.096646      -0.058388      -0.006543       0.285164   \n",
       "\n",
       "                  total_word_count  A2_word_%  B1_word_%  B2_word_%  C1_word_%  \n",
       "A2_word_count             0.966391  -0.072345  -0.069939  -0.249865  -0.096646  \n",
       "B1_word_count             0.934342  -0.144309   0.151579  -0.183528  -0.058388  \n",
       "B2_word_count             0.893735  -0.096822   0.063359   0.023103  -0.006543  \n",
       "C1_word_count             0.887569  -0.217707  -0.073763  -0.232509   0.285164  \n",
       "total_word_count          1.000000  -0.259290  -0.170055  -0.361024  -0.124797  \n",
       "A2_word_%                -0.259290   1.000000   0.407702   0.549889   0.082971  \n",
       "B1_word_%                -0.170055   0.407702   1.000000   0.531400   0.319421  \n",
       "B2_word_%                -0.361024   0.549889   0.531400   1.000000   0.266357  \n",
       "C1_word_%                -0.124797   0.082971   0.319421   0.266357   1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2XGf_EE4rUs"
   },
   "source": [
    "**Выбираем наименее коррелирующие между собой признаки, которые будут участвовать в обучении модели - это признаки процентного соотношения.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xVZH4YY4rUs"
   },
   "source": [
    "### Выбор модели обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al0pNo8X4rUt"
   },
   "source": [
    "**Обучающей моделью выбираем  Многоклассовую Логистическую регрессию.**\n",
    "\n",
    "**Делим данные на обучающие и целевой признак, внутри кросс-валидации делим данные на тренировочную, валидационную и тестовую выборки, подбираем гиперпараметры, производим обучение, тестирование модели и выбираем модель с лучшими гиперпараметрами. Метрикой качества выбираем Accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:30:44.734512Z",
     "iopub.status.busy": "2023-08-15T18:30:44.733880Z",
     "iopub.status.idle": "2023-08-15T18:31:44.052462Z",
     "shell.execute_reply": "2023-08-15T18:31:44.051041Z",
     "shell.execute_reply.started": "2023-08-15T18:30:44.734461Z"
    },
    "id": "MEku2PDP4rUt",
    "outputId": "120637f9-f045-41a6-af4d-fccc79309c1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [00:13<00:55, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Accuracy: 0.6909090909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.50      0.25      0.33         4\n",
      "          B1       0.56      0.60      0.58        15\n",
      "          B2       0.75      0.75      0.75        28\n",
      "          C1       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.69        55\n",
      "   macro avg       0.65      0.62      0.62        55\n",
      "weighted avg       0.68      0.69      0.68        55\n",
      "\n",
      "Test Accuracy: 0.6181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.33      0.12      0.18         8\n",
      "          B1       0.40      0.50      0.44        12\n",
      "          B2       0.68      0.78      0.72        27\n",
      "          C1       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.62        55\n",
      "   macro avg       0.60      0.54      0.55        55\n",
      "weighted avg       0.61      0.62      0.60        55\n",
      "\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:22<00:32, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Accuracy: 0.7090909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.50      0.25      0.33         4\n",
      "          B1       0.69      0.69      0.69        16\n",
      "          B2       0.71      0.81      0.76        27\n",
      "          C1       0.83      0.62      0.71         8\n",
      "\n",
      "    accuracy                           0.71        55\n",
      "   macro avg       0.68      0.59      0.62        55\n",
      "weighted avg       0.71      0.71      0.70        55\n",
      "\n",
      "Test Accuracy: 0.6181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.50      0.12      0.20         8\n",
      "          B1       0.41      0.58      0.48        12\n",
      "          B2       0.69      0.74      0.71        27\n",
      "          C1       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.62        55\n",
      "   macro avg       0.61      0.55      0.55        55\n",
      "weighted avg       0.63      0.62      0.60        55\n",
      "\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:30<00:19,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Accuracy: 0.5818181818181818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.00      0.00      0.00         8\n",
      "          B1       0.50      0.62      0.55        13\n",
      "          B2       0.59      0.79      0.68        24\n",
      "          C1       1.00      0.50      0.67        10\n",
      "\n",
      "    accuracy                           0.58        55\n",
      "   macro avg       0.52      0.48      0.47        55\n",
      "weighted avg       0.56      0.58      0.55        55\n",
      "\n",
      "Test Accuracy: 0.6181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       1.00      0.14      0.25         7\n",
      "          B1       0.40      0.46      0.43        13\n",
      "          B2       0.66      0.85      0.74        27\n",
      "          C1       1.00      0.50      0.67         8\n",
      "\n",
      "    accuracy                           0.62        55\n",
      "   macro avg       0.76      0.49      0.52        55\n",
      "weighted avg       0.69      0.62      0.59        55\n",
      "\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:39<00:09,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Accuracy: 0.5636363636363636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.00      0.00      0.00         7\n",
      "          B1       0.36      0.38      0.37        13\n",
      "          B2       0.62      0.77      0.69        26\n",
      "          C1       0.86      0.67      0.75         9\n",
      "\n",
      "    accuracy                           0.56        55\n",
      "   macro avg       0.46      0.46      0.45        55\n",
      "weighted avg       0.52      0.56      0.54        55\n",
      "\n",
      "Test Accuracy: 0.7407407407407407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.75      0.43      0.55         7\n",
      "          B1       0.67      0.46      0.55        13\n",
      "          B2       0.72      0.96      0.83        27\n",
      "          C1       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.74        54\n",
      "   macro avg       0.78      0.64      0.69        54\n",
      "weighted avg       0.75      0.74      0.72        54\n",
      "\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Accuracy: 0.6727272727272727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.00      0.00      0.00         6\n",
      "          B1       0.46      0.55      0.50        11\n",
      "          B2       0.69      0.89      0.78        28\n",
      "          C1       1.00      0.60      0.75        10\n",
      "\n",
      "    accuracy                           0.67        55\n",
      "   macro avg       0.54      0.51      0.51        55\n",
      "weighted avg       0.63      0.67      0.63        55\n",
      "\n",
      "Test Accuracy: 0.6481481481481481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A2       0.50      0.14      0.22         7\n",
      "          B1       0.53      0.67      0.59        12\n",
      "          B2       0.66      0.78      0.71        27\n",
      "          C1       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.65        54\n",
      "   macro avg       0.67      0.55      0.57        54\n",
      "weighted avg       0.66      0.65      0.63        54\n",
      "\n",
      "Average Accuracy: 0.6436363636363637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_text = df_1['lemmatized_text']\n",
    "X_num = df_1[['A2_word_%', 'B1_word_%', 'B2_word_%', 'C1_word_%']]\n",
    "y = df_1['level']\n",
    "\n",
    "# Создаем экземпляр StratifiedKFold для кросс-валидации\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Инициализируем пустой список для хранения результатов\n",
    "results = []\n",
    "best_test_params = 0\n",
    "best_params_gs_lr = []\n",
    "\n",
    "# Итерируемся по разбиениям кросс-валидации\n",
    "for trainval_index, test_index in tqdm(cv.split(X_text, y), total=cv.get_n_splits()):\n",
    "    X_text_trainval, X_text_test = X_text.iloc[trainval_index], X_text.iloc[test_index]\n",
    "    X_num_trainval, X_num_test = X_num.iloc[trainval_index], X_num.iloc[test_index]\n",
    "    y_trainval, y_test = y.iloc[trainval_index], y.iloc[test_index]\n",
    "\n",
    "    # Дополнительно разбиваем временную выборку на тренировочную и валидационную\n",
    "    X_text_train, X_text_val, X_num_train, X_num_val, y_train, y_val = train_test_split(\n",
    "        X_text_trainval, X_num_trainval, y_trainval, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Создаем TfidfVectorizer для текстовых данных\n",
    "    tfidf_vect = TfidfVectorizer(use_idf=True, norm='l2', smooth_idf=True,\n",
    "                                 strip_accents=None, lowercase=False, preprocessor=None)\n",
    "    X_text_train_tfidf = tfidf_vect.fit_transform(X_text_train)\n",
    "    X_text_val_tfidf = tfidf_vect.transform(X_text_val)\n",
    "    X_text_test_tfidf = tfidf_vect.transform(X_text_test)\n",
    "\n",
    "    # Объединяем текстовые и числовые признаки для текущего фолда\n",
    "    X_train_combined = hstack([X_text_train_tfidf, X_num_train])\n",
    "    X_val_combined = hstack([X_text_val_tfidf, X_num_val])\n",
    "    X_test_combined = hstack([X_text_test_tfidf, X_num_test])\n",
    "\n",
    "    # Создаем мультиклассовую логистическую регрессию\n",
    "    lr = LogisticRegression(random_state=42, multi_class='ovr')\n",
    "\n",
    "    # Задаем сетку параметров для поиска по сетке\n",
    "    param_grid = [{'penalty': ['l1', 'l2'],\n",
    "                   'C': [1.0, 10.0, 100.0],\n",
    "                   'solver': ['lbfgs', 'liblinear']}]\n",
    "\n",
    "    # Инициализируем GridSearchCV\n",
    "    gs_lr_tfidf = GridSearchCV(lr, param_grid,\n",
    "                               scoring='accuracy',\n",
    "                               cv=5,\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    # Обучаем модель на тренировочных данных с использованием GridSearchCV\n",
    "    gs_lr_tfidf.fit(X_train_combined, y_train)\n",
    "\n",
    "    # Получаем лучшие параметры и оцениваем производительность модели на валидационной выборке\n",
    "    best_params = gs_lr_tfidf.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Делаем предсказания на валидационных данных\n",
    "    y_pred = gs_lr_tfidf.predict(X_val_combined)\n",
    "\n",
    "    # Оцениваем производительность модели на валидационной выборке\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Выводим отчет по каждому целевому признаку\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    # Делаем предсказания на тестовой выборке и оцениваем качество модели\n",
    "    y_pred_test = gs_lr_tfidf.predict(X_test_combined)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Test Accuracy:\", accuracy_test)\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "    # Добавляем результаты в список\n",
    "    results.append(accuracy)\n",
    "\n",
    "    if accuracy_test > best_test_params:\n",
    "        best_test_params = accuracy_test\n",
    "        best_params_gs_lr = gs_lr_tfidf.best_params_\n",
    "\n",
    "# Анализируем результаты кросс-валидации\n",
    "average_accuracy = np.mean(results)\n",
    "print(\"Average Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:37:06.276735Z",
     "iopub.status.busy": "2023-08-15T18:37:06.276124Z",
     "iopub.status.idle": "2023-08-15T18:37:06.285852Z",
     "shell.execute_reply": "2023-08-15T18:37:06.284221Z",
     "shell.execute_reply.started": "2023-08-15T18:37:06.276695Z"
    },
    "id": "sn1mhDhL4rUt",
    "outputId": "64a77e93-ba32-4180-b70e-97d892c1eacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'C': 100.0, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      "Best Accuracy: 0.741\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % best_params_gs_lr)\n",
    "print('Best Accuracy: %.3f' % best_test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHYYTMnd4rUu"
   },
   "source": [
    "**Сохраняем лучшую модель и TfidfVectorizer для получения предсказаний на новых данных.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:39:46.022489Z",
     "iopub.status.busy": "2023-08-15T18:39:46.021957Z",
     "iopub.status.idle": "2023-08-15T18:39:46.036324Z",
     "shell.execute_reply": "2023-08-15T18:39:46.034995Z",
     "shell.execute_reply.started": "2023-08-15T18:39:46.022451Z"
    },
    "id": "dMwiOroP4rUu",
    "outputId": "706e40ef-b249-40d6-cfbd-10b569e7f2ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_logistic_regression_model.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получение экземпляра лучшей модели\n",
    "best_lr_model = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "# Сохранение модели в файл\n",
    "model_filename = 'best_logistic_regression_model.joblib'\n",
    "joblib.dump(best_lr_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:39:49.598577Z",
     "iopub.status.busy": "2023-08-15T18:39:49.597112Z",
     "iopub.status.idle": "2023-08-15T18:39:49.890200Z",
     "shell.execute_reply": "2023-08-15T18:39:49.888953Z",
     "shell.execute_reply.started": "2023-08-15T18:39:49.598520Z"
    },
    "id": "Sar79EKR4rUu",
    "outputId": "a02c3d44-0b9f-4937-f3d2-e147eda0252c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_lr_model.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраняем TfidfVectorizer и обученную модель\n",
    "joblib.dump(tfidf_vect, 'tfidf_vect.pkl')\n",
    "joblib.dump(gs_lr_tfidf, 'best_lr_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm8G-cjO4rUv"
   },
   "source": [
    "## Получение предсказаний на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvEHS4QA4rUv"
   },
   "source": [
    "**Для предсказания категории сложности фильма загружаем выбранный файл субтитров с разрешением `.srt`. В нашем случае выбираем один из файлов из папки `Subtitles`, которым не была присвоена категория сложности, и которые мы удалили из таблицы до обучения модели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T18:40:06.525750Z",
     "iopub.status.busy": "2023-08-15T18:40:06.525312Z",
     "iopub.status.idle": "2023-08-15T18:40:07.753447Z",
     "shell.execute_reply": "2023-08-15T18:40:07.752187Z",
     "shell.execute_reply.started": "2023-08-15T18:40:06.525717Z"
    },
    "id": "BjIcrFPO4rUv",
    "outputId": "34ce224c-ef0a-4e6e-ad01-e1b3c5c6fc15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фильм соответствует категории сложности английского языка: ['A2']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = joblib.load('tfidf_vect.pkl')\n",
    "loaded_best_lr_model = joblib.load('best_lr_model.pkl')\n",
    "\n",
    "# Функция для предобработки текста субтитров\n",
    "def preprocess_subtitle(subtitle_text):\n",
    "    # Применяем предобработку и лемматизацию как в обучении модели\n",
    "    processed_text = prepare_text(subtitle_text)\n",
    "    clean_text = del_stopwords(processed_text)\n",
    "    lemmatized_text = ''.join(lemmatize(clean_text))\n",
    "    return lemmatized_text\n",
    "\n",
    "# Загружаем модель из файла\n",
    "loaded_best_lr_model = joblib.load(model_filename)\n",
    "\n",
    "# Путь к файлу .srt\n",
    "file_path = \"D:/Яндекс Практикум/17. Мастерская_2/Датасет/Subtitles_all/Subtitles/The_man_called_Flintstone(1966).srt\"\n",
    "\n",
    "# Определяем кодировку файла\n",
    "encoding = detect_encoding(file_path)\n",
    "\n",
    "# Открываем файл .srt с определенной кодировкой\n",
    "with open(file_path, 'r', encoding=encoding) as file:\n",
    "    subtitle_lines = file.readlines()\n",
    "\n",
    "# Обрабатываем файл .srt и извлекаем текст субтитров\n",
    "subtitles = []\n",
    "current_subtitle = \"\"\n",
    "for line in subtitle_lines:\n",
    "    line = line.strip()\n",
    "    if line.isdigit() or line == \"\":\n",
    "        # Пропускаем номер субтитра и пустые строки\n",
    "        continue\n",
    "    elif \"-->\" in line:\n",
    "        # Пропускаем временные коды\n",
    "        continue\n",
    "    else:\n",
    "        current_subtitle += \" \" + line\n",
    "\n",
    "        if line.endswith(\".\"):\n",
    "            # Конец субтитра, добавляем его в список\n",
    "            subtitles.append(current_subtitle.strip())\n",
    "            current_subtitle = \"\"\n",
    "\n",
    "# Объединяем текст субтитров в одну строку\n",
    "full_subtitle_text = ' '.join(subtitles)\n",
    "\n",
    "# Предобрабатываем текст субтитров\n",
    "preprocessed_subtitle_text = preprocess_subtitle(full_subtitle_text)\n",
    "\n",
    "# Преобразуем предобработанный текст с помощью ранее обученного TfidfVectorizer\n",
    "new_text_tfidf = tfidf_vect.transform([preprocessed_subtitle_text])\n",
    "\n",
    "# Создание массива числовых признаков для новых данных (аналогично тому, что использовалось для обучения модели)\n",
    "new_A2_word_count = count_words(preprocessed_subtitle_text, A2_words)\n",
    "new_B1_word_count = count_words(preprocessed_subtitle_text, B1_words)\n",
    "new_B2_word_count = count_words(preprocessed_subtitle_text, B2_words)\n",
    "new_C1_word_count = count_words(preprocessed_subtitle_text, C1_words)\n",
    "new_total_word_count = len(preprocessed_subtitle_text.split())\n",
    "\n",
    "new_A2_word_percent = new_A2_word_count / new_total_word_count\n",
    "new_B1_word_percent = new_B1_word_count / new_total_word_count\n",
    "new_B2_word_percent = new_B2_word_count / new_total_word_count\n",
    "new_C1_word_percent = new_C1_word_count / new_total_word_count\n",
    "\n",
    "new_numerical_features = np.array([[new_A2_word_count, new_B1_word_count, new_B2_word_count, new_C1_word_count,\n",
    "                                  new_total_word_count, new_A2_word_percent, new_B1_word_percent, new_B2_word_percent,\n",
    "                                  new_C1_word_percent]])\n",
    "\n",
    "new_numerical_features_selected = new_numerical_features[:, 5:9]\n",
    "\n",
    "# Объединяем текстовые и числовые признаки\n",
    "new_combined_features = hstack([new_text_tfidf, new_numerical_features_selected])\n",
    "\n",
    "# Применяем модель к новым данным\n",
    "predicted_labels = loaded_best_lr_model.predict(new_combined_features)\n",
    "print('Фильм соответствует категории сложности английского языка:', predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X8UN37I4rUv"
   },
   "source": [
    "**Файлу субтитров присвоена категория по результатам предсказания.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLEpVrTw4rUw"
   },
   "source": [
    "## Вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTYX6xFN4rUw"
   },
   "source": [
    "**Для выполнения поставленной задачи - разработать ML решение для автоматического определения уровня сложности англоязычных фильмов - произвели изучение, предобработку предоставленных данных, создали новые признаки, обучили модель Логистическая регрессия, получили Accuracy 0,741. С помощью этой модели разработали форму и получили предсказание категории сложности англоязычного фильма по тексту субтитров. Задача выполнена.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
